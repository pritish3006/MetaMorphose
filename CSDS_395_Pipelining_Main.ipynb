{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/tensorflow/examples.git"
      ],
      "metadata": {
        "id": "UVU5zLxU9Sdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_examples.models.pix2pix import pix2pix"
      ],
      "metadata": {
        "id": "uqQ8bqLQ4shK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7--hLI_t8-85"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"/content/goblin_images.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/new_folder\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar --gunzip --extract --verbose --file=lfw-funneled.tgz"
      ],
      "metadata": {
        "id": "0hHkmdjx43BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goblin_dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=\"new_folder\", label_mode=None, image_size=(512,512), batch_size=1, shuffle=True\n",
        ")\n",
        "people_dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=\"lfw_funneled\", label_mode=None, image_size=(512,512), batch_size=1, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "95eDiVEt5y6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Goblin_dataset_size = 750\n",
        "people_dataset_size = 13233\n",
        "goblin_train_size = int(0.7 * Goblin_dataset_size)\n",
        "goblin_test_size = 750 - goblin_train_size\n",
        "people_train_size = int(0.7 * people_dataset_size)\n",
        "people_test_size = 13233 - people_train_size\n",
        "BUFFER_SIZE = 100\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 512\n",
        "IMG_HEIGHT = 512"
      ],
      "metadata": {
        "id": "4YXxIZMXAibA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the goblin and people train data set should never touch otherwise one ends up with 4 dimensions and the other ends up with 5\n",
        "#I don't know and I won't pretend to\n",
        "goblin_train_dataset = goblin_dataset.take(goblin_train_size)\n",
        "goblin_test_dataset = goblin_dataset.skip(goblin_train_size)"
      ],
      "metadata": {
        "id": "UqoBylqiA3we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goblin_train_dataset"
      ],
      "metadata": {
        "id": "DJRZKQ2mXjNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_train_dataset = people_dataset.take(people_train_size)\n",
        "people_test_dataset = people_dataset.skip(people_train_size)"
      ],
      "metadata": {
        "id": "AFkSrWzUAH0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "people_train_dataset"
      ],
      "metadata": {
        "id": "O36PKP_gXlD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#up to the exact same in cyclic GAN\n",
        "sample_person = next(iter(people_train_dataset))"
      ],
      "metadata": {
        "id": "6ugeUkKYuJ3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#doesn't work\n",
        "def random_crop(image):\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image"
      ],
      "metadata": {
        "id": "Owtv0HT6hthP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shouldn't work\n",
        "random_crop(next(iter(goblin_dataset))[0])"
      ],
      "metadata": {
        "id": "KxG5wxlhXQrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  #keeping it between 1 and -1\n",
        "  image = (image / 127.5) - 1\n",
        "  return image"
      ],
      "metadata": {
        "id": "SYSUJdmPiNd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_jitter(image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  image = tf.image.resize(image, [540, 540],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  image = random_crop(image)\n",
        "\n",
        "  # random mirroring\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "  return image"
      ],
      "metadata": {
        "id": "5Un75mOjiP-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#returns a jittered image as a process of numbers\n",
        "def preprocess_image_train(image, label):\n",
        "  image = random_jitter(image)\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "metadata": {
        "id": "95m-qEv4iSjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_test(image):\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "metadata": {
        "id": "7NK1NBhzXV5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "bg4Pe2u8XXq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize works\n",
        "#random_crop and random_jitter doesn't work\n",
        "goblin_train_dataset = goblin_train_dataset.cache().map(\n",
        "    normalize, num_parallel_calls=AUTOTUNE).shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "goblin_test_dataset = goblin_test_dataset.cache().map(\n",
        "    normalize, num_parallel_calls=AUTOTUNE).shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "people_train_dataset = people_train_dataset.map(\n",
        "    normalize, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "people_test_dataset = people_test_dataset.map(\n",
        "    normalize, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "JUdYlhAxilzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_goblin = next(iter(goblin_train_dataset))\n",
        "sample_person = next(iter(people_train_dataset))\n",
        "sample_goblin"
      ],
      "metadata": {
        "id": "jnX9Hsc6XbFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}